{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ICCV 2023 Tutorial] Sharon Yixuan Li: Out-of-Distribution detection\n",
    "\n",
    "- https://abursuc.github.io/many-faces-reliability/\n",
    "- https://abursuc.github.io/many-faces-reliability/slides/2023_iccv_reliability_sharon_ood.pdf\n",
    "- https://youtu.be/hgLC9_9ZCJI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/001.jpg\" style=\"max-height: 400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginal distribution (In distribution; ID): $p^{\\text{in}}(\\mathbf x)$ \\\n",
    "Input space: $\\mathcal X = \\R^d$ \\\n",
    "Label space: $\\mathcal Y = \\{1, -1\\}$\n",
    "\n",
    "Novel distribution (Out of distribution; OOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "High-capacity neural networks exacerbate __over-confident__ predictions\n",
    "\n",
    "(Left) In-distribution: mixture of 3 gaussians \\\n",
    "(Right) Decision boundary learned by MLP: overconfident in red regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/002.png\" style=\"max-height: 250px; margin: 10px\" />\n",
    "<img src=\"src/003.png\" style=\"max-height: 250px; margin: 10px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial\n",
    "\n",
    "- Inference-time OOD detection\n",
    "  - Output-based methods\n",
    "  - Distance-based methods\n",
    "- Training-time regularization for OOD detection\n",
    "  - Safety-aware learning objective\n",
    "  - Synthesizing virtual outliers\n",
    "  - Leveraging wild unlabeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical risk minimization\n",
    "\n",
    "V. Vapnik. Principles of risk minimization for learning theory. NIPS 1991\n",
    "\n",
    "The objective of the basic training method (what we all know)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Estimation Model\n",
    "\n",
    "The learning process is described through three components:\n",
    "1. A generator of random vector $x$, drawn independently from a fixed but unknown distrubution $P(x)$.\n",
    "1. A supervisor which returns an output vector $y$ to every input vector $x$, according to a condiitonal distribution function $P(y|x)$, also fixed but unknown.\n",
    "1. A learning machine capable of implementing a set of function $f(x;w), w \\in W$.\n",
    "\n",
    "The problem of learning is that of choosing from the given set of functions the one which approximates best the supervisor's response. The selecition is based on a training set of $n$ independent observations:\n",
    "\n",
    "$$\n",
    "(x_1, x_2), \\cdots, (x_n, y_n) \\qquad \\cdots (1)\n",
    "$$\n",
    "\n",
    "The formulation given above implies that learning corresponds to the problem of function approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem of Risk ($R$) Minimization\n",
    "\n",
    "In order to choose the best available approximation to the supervisor's response, we measure the loss or discrepancy $L(y, f(x;w))$ between the response $y$ of the supervisor to a given input $x$ and the response $f(x, w)$ provided by the learning machine.\n",
    "\n",
    "$$\n",
    "R(w) = \\int L(y, f(x; w)) dP(x, y) \\qquad \\cdots (2)\n",
    "$$\n",
    "\n",
    "The goal is to minimize the risk functional $R(w)$ over the class of functions $f(x; w), w \\in W$.\n",
    "But the joint probability distribution $P(x,y) = P(y|x)P(x)$ is unknown and the only available information is contained in the training set (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Empirical Risk ($E$) Minimization\n",
    "\n",
    "In order to solve this problem, the following induction principle is proposed: the risk functional $R(w)$ is replaced by the empirical risk functional\n",
    "\n",
    "$$\n",
    "E(w) = \\dfrac{1}{n} \\sum \\limits_{i=1}^{n} L(y_i, f(x_i; w))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
